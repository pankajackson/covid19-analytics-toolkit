---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "coviddatapuller.name" . }}
data:
  requirements.txt: |
    elasticsearch=={{ .Values.elasticsearch.version }}
    retrying==1.3.3
    requests==2.27.1
    geopy==2.2.0
    pandas==1.3.5
    pycountry==22.1.10


  data_puller.py: |
    import pandas as pd
    import numpy as np
    from elasticsearch import Elasticsearch, helpers
    from geopy.geocoders import Nominatim
    from datetime import datetime
    from retrying import retry
    from urllib.parse import urlparse
    import requests
    import os
    import pycountry
    import sys

    es_index = os.getenv('ES_INDEX', "covid")
    print('ES_INDEX: %s' % es_index)
    owid_data_source_url = os.getenv('OWID_SOURCE_DATA_URL', "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv")
    csse_data_source_baseurl = os.getenv('CSSEGISANDDATA_DATA_SOURCE_BASEURL', "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports")
    source_data_dir = os.getenv('SOURCE_DATA_DIR', 'data/raw/')
    dest_clean_data_dir = os.getenv('DEST_CLEAN_DATA_DIR', 'data/cleaned/')
    es_hosts = os.getenv('ES_HOSTS', ['https://es.jackson.com'])
    es_user = os.getenv('ES_USER', 'elastic')
    es_password = os.getenv('ES_PASSWORD', '7876zXNzlsn66puM0uS4UN68')
    es_scheme = os.getenv('ES_SCHEME', 'https')
    es_ssl_ca = os.getenv('ES_CA', 'cert/ca.crt')
    es_verify_certs = os.getenv("ES_VERIFY_CERT", 'False').lower() in ('yes', 'true', '1', 't', 'y')
    geo_points = {}
    iso_codes = {}

    # Get ES
    def get_es_con():
        es = Elasticsearch(
            hosts=es_hosts,
            http_auth=(es_user, es_password),
            scheme=es_scheme,
            timeout=10,
            ca_certs=es_ssl_ca,
            verify_certs=es_verify_certs,
        )
        return es

    def get_paths(url):
        timestamp = datetime.now().strftime("%Y%m%d__%H")
        file_name = os.path.basename(urlparse(url).path)
        if not os.path.exists(os.path.abspath(source_data_dir)):
            os.makedirs(os.path.abspath(source_data_dir))
        raw_file_path = os.path.join(os.path.abspath(source_data_dir), timestamp + '_' + file_name)
        if not os.path.exists(os.path.abspath(dest_clean_data_dir)):
            os.makedirs(os.path.abspath(dest_clean_data_dir))
        cleaned_file_path = os.path.join(os.path.abspath(dest_clean_data_dir), timestamp + '_cleaned_' + file_name)
        return {
            "url": url,
            "url_file_name": file_name,
            "raw_file_path": raw_file_path,
            "raw_file_name": os.path.basename(raw_file_path),
            "raw_file_dir_name": os.path.dirname(raw_file_path),
            "cleaned_file_path": cleaned_file_path,
            "cleaned_file_name": os.path.basename(cleaned_file_path),
            "cleaned_file_dir_name": os.path.dirname(cleaned_file_path)
        }


    # Download Data
    @retry(stop_max_attempt_number=3, wait_fixed=10000)
    def download_data(url):
        try:
            print('Downloading Data %s...' % url)
            paths = get_paths(url)
            if not os.path.exists(os.path.abspath(paths['raw_file_dir_name'])):
                os.makedirs(os.path.abspath(paths['raw_file_dir_name']))
            resp = requests.get(url)
            if resp.status_code == 200:
                file = open(paths['raw_file_path'], 'wb')
                file.write(resp.content)
                file.close()
                return paths['raw_file_path']
            else:
                print('Error Downloading data from %s\nError Code %s' % (url, resp.status_code))
                return None
        except Exception as e:
            print(str(e))
            return None

    # Load DataSet
    def get_dataframe(path):
        print('Reading Data...')
        df = pd.read_csv(path)
        return df


    def get_lat_long(combined=None, country=None, state=None, iso_code=None):
        if not combined and not iso_code:
            combined = str(state) + ',' + str(country)
        if combined and combined in geo_points.keys():
            return geo_points[combined]
        if iso_code and iso_code in geo_points.keys():
            return geo_points[iso_code]
        if iso_code:
            print('Fetching LAT LONG for %s' % str(iso_code))
        else:
            print('Fetching LAT LONG for %s' % str(combined))
        loc = None
        geolocator = Nominatim(user_agent="python")
        if country and state:
            loc = geolocator.geocode(state + ',' + country)
        if not loc and combined:
            loc = geolocator.geocode(combined)
        if not loc and state:
            loc = geolocator.geocode(state)
        if not loc and country:
            loc = geolocator.geocode(country)
        if not loc and iso_code:
            try:
                c = None
                if '-' in iso_code:
                    c = pycountry.subdivisions.get(code=iso_code)
                elif len(iso_code) == 2:
                    c = pycountry.countries.get(alpha_2=iso_code)
                elif len(iso_code) == 3:
                    c = pycountry.countries.get(alpha_3=iso_code)
                if c:
                    loc = geolocator.geocode(c.name)
            except:
                pass
        if not loc:
            print('Location Not Found for %s, %s' % (state, country))
            return None
        else:
            # Update Local geolocation database
            if iso_code:
                geo_points[iso_code] = {
                    'latitude': loc.latitude,
                    'longitude': loc.longitude
                }
                return geo_points[iso_code]
            else:
                geo_points[combined] = {
                    'latitude': loc.latitude,
                    'longitude': loc.longitude
                }
                return geo_points[combined]


    def get_iso_code(country, state=None, combined=None):
        if country in iso_codes.keys():
            return iso_codes[country]
        print('Fetching ISO_CODE for %s' % str(country))
        geolocator = Nominatim(user_agent="python")
        loc = None
        if country and state:
            loc = geolocator.geocode(str(state) + ',' + str(country))
        if not loc and combined:
            loc = geolocator.geocode(str(combined))
        if not loc and country:
            loc = geolocator.geocode(str(country))
        if not loc and state:
            loc = geolocator.geocode(str(state))
        try:
            location = pycountry.countries.get(alpha_2=str(loc.raw['address']['country_code']))
            iso_code = {
                'alpha_3': str(location.alpha_3).upper(),
                'alpha_2': str(location.alpha_2).upper(),
            }
            iso_codes[country] = iso_code
            return iso_code
        except:
            try:
                location = pycountry.countries.search_fuzzy(str(country))
            except:
                try:
                    location = pycountry.countries.search_fuzzy(str(loc.address).split(',')[-1].strip())
                except:
                    try:
                        location = pycountry.countries.search_fuzzy(str(combined))
                    except:
                        try:
                            location = pycountry.countries.search_fuzzy(str(state))
                        except:
                            res = {'alpha_3': '', 'alpha_2': ''}
                            return res
        iso_code = {
            'alpha_3': str(location[0].alpha_3).upper(),
            'alpha_2': str(location[0].alpha_2).upper(),
        }
        iso_codes[country] = iso_code
        return iso_code

    # Save data in CSV
    def save_cleaned_df(df, file_path):
        print('Saving Cleaned Data...')
        if not os.path.exists(os.path.abspath(os.path.dirname(file_path))):
            os.makedirs(os.path.abspath(os.path.dirname(file_path)))
        df.to_csv(file_path)

    # Fill csse missing values
    def get_cleaned_csse_data_df(df):
        print('Cleaning CSSE Data...')

        if 'Lat' not in df:
            df['Lat'] = np.nan
        if 'Long_' not in df:
            df['Long_'] = np.nan
        if 'Province/State' in df and not 'Province_State' in df:
            df.rename(columns={'Province/State': 'Province_State'}, inplace=True)
        if 'Country/Region' in df and not 'Country_Region' in df:
            df.rename(columns={'Country/Region': 'Country_Region'}, inplace=True)
        if 'Last Update' in df and not 'Last_Update' in df:
            df.rename(columns={'Last Update': 'Last_Update'}, inplace=True)
        df.drop(columns='Last_Update', inplace=True)
        if 'Case_Fatality_Ratio' in df:
            df.drop(columns='Case_Fatality_Ratio', inplace=True)
        if not 'Combined_Key' in df:
            if 'Province_State' in df and 'Country_Region' in df:
                df['Combined_Key'] = df['Province_State'] + ', ' + df['Country_Region']
            if  'Country_Region' in df:
                df['Combined_Key'] = df['Country_Region']
            if 'Province_State' in df:
                df['Combined_Key'] = df['Province_State']

        # Filling iso_code
        df['iso_code'] = np.nan
        for i, row in df.groupby('Country_Region'):
            for index in range(0, row.shape[0]-1):
                location = row.iloc[index]
                country = location['Country_Region']
                state = location['Province_State']
                combined = location['Combined_Key']
                iso_code = get_iso_code(country=country, state=state, combined=combined)
                if iso_code['alpha_3']:
                    df.loc[df.Country_Region == i, 'iso_code'] = iso_code['alpha_3']
                    break


        # Filling missing latitude and longitude
        for i, row in df[df['Lat'].isnull()].groupby('Combined_Key'):
            for index in range(0, row.shape[0]-1):
                location = row.iloc[index]
                country = location['Country_Region']
                state = location['Province_State']
                combined = location['Combined_Key']
                geo_location = get_lat_long(country=country, state=state, combined=combined)
                if geo_location:
                    df.loc[df.Combined_Key == i, 'Lat'] = geo_location['latitude']
                    df.loc[df.Combined_Key == i, 'Long_'] = geo_location['longitude']
                    break


        # Fill Missing Numerical features
        numeric_data = df.select_dtypes(include=['int64', 'float64'])
        df_filled_zero = numeric_data.fillna(0)
        df.update(df_filled_zero)

        # Fill Missing Categorical features
        categorical_data = df.select_dtypes(include=['object'])
        df_filled_unknown = categorical_data.fillna('Unknown')
        df.update(df_filled_unknown)


        return df

    def get_csse_df(date=datetime.now()):
        file_format = date.strftime('%m-%d-%Y')
        csse_data_source_url = os.path.join(csse_data_source_baseurl, '%s.csv' % file_format)
        paths = get_paths(csse_data_source_url)
        if not os.path.exists(paths['cleaned_file_path']):
            if not os.path.exists(paths['raw_file_path']):
                csse_df_csv = download_data(csse_data_source_url)
            else:
                csse_df_csv = paths['raw_file_path']
            if not csse_df_csv:
                return pd.DataFrame()
            csse_df = get_dataframe(csse_df_csv)
            cleaned_csse_df = get_cleaned_csse_data_df(csse_df)
            save_cleaned_df(cleaned_csse_df, paths['cleaned_file_path'])
        else:
            cleaned_csse_df = pd.read_csv(paths['cleaned_file_path'])
        return cleaned_csse_df


    # Fill  owid missing values
    def get_cleaned_owid_df(df):
        print('Cleaning OWID Data...')

        # Fill missing continent
        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
        iso_codes = df['iso_code'].unique()
        for iso_c in iso_codes:
            # Drop Data having iso_code starting with OWID_
            if str(iso_c).startswith('OWID_'):
                index = df[df['iso_code']==iso_c].index
                df.drop(index=index, inplace=True)
            else:
                null_count = df[df['iso_code']==iso_c]['continent'].isnull().sum()
                if null_count > 0:
                    print(iso_c, 'has %s null continent' % null_count)
                    n_null = df[df.continent.notnull()]
                    con = n_null[n_null['iso_code']==iso_c].head()['continent'].unique()
                    if len(con) != 0:
                        df[df['iso_code'] == iso_c]['continent'].fillna(con[0], inplace=True)
                        print(iso_c, ' has filled with continent %s' % con[0])
                    else:
                        print(iso_c, ' has no continent')

                    print(df['continent'].isnull().sum())

        # Fill Missing tests_units
        df['tests_units'].fillna('None', inplace=True)

        # Fill Missing Numerical Fields
        numeric_data = df.select_dtypes(include=['int64', 'float64'])

        # Fill NA using FFill Value or Zero Value
        df_filled_zero = numeric_data.fillna(0)
        df.update(df_filled_zero)
        return df

    @retry(stop_max_attempt_number=3, wait_fixed=10000)
    def push_bulk_data_to_es(actions):
        print('Pushing %s data to Elasticsearch...' % len(actions))
        es = get_es_con()
        if actions:
            res = helpers.bulk(es, actions=actions)
            return res
        return None

    def get_last_updated_datetime(es=get_es_con(), iso_c=None):
        if iso_c:
            latest_data_query_body = {"sort": [{"@timestamp": {"order": "desc"}}], "query": {"match": {"iso_code": iso_c}}}

            latest_data = es.search(index=es_index + '-*', sort=[{"@timestamp": {"order": "desc"}}], query={"match": {"iso_code": iso_c}}, size=1)
            if len(latest_data['hits']['hits']) > 0:
                latest_date = latest_data['hits']['hits'][0]['_source']['date']
                return latest_date
        return None

    @retry(stop_max_attempt_number=3, wait_fixed=10000)
    def process_bulk_es_data(df):
        print('Processing bulk Data...')
        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
        actions = []
        for iso_c, data in df.groupby('iso_code'):
            # try:
            data['date'] = data['date'].dt.to_pydatetime()
            data['date'] = data['date'].dt.strftime('%Y-%m-%d %H:%M:%S')
            last_updated_datetime = get_last_updated_datetime(iso_c=iso_c)
            if last_updated_datetime:
                data=data[data['date']>=last_updated_datetime]
            df_dict = data.to_dict(orient="records")
            for doc in df_dict:
                doc['@timestamp'] = datetime.strptime(doc['date'], '%Y-%m-%d %H:%M:%S')
                iso_geolocation = get_lat_long(iso_code=doc['iso_code'])
                if iso_geolocation:
                    doc['geo_point'] = {
                        'lat': iso_geolocation['latitude'],
                        'lon': iso_geolocation['longitude'],
                    }
                index = (es_index + '-' + str(doc['@timestamp'].year) + '-' + str(doc['@timestamp'].month)).lower()
                csse_df = get_csse_df(doc['@timestamp'])
                if not csse_df.empty:
                    csse_df_cont = csse_df[csse_df['iso_code'] == str(doc['iso_code']).upper()]
                    if csse_df_cont.shape[0] != 0:
                        for csse_doc in csse_df_cont.to_dict(orient="records"):
                            _id = (doc['continent'] + doc['iso_code'] + '_' + str(doc['@timestamp']) + '_' +  str(csse_doc['Lat']) + '_' +  str(csse_doc['Long_']) + '_' +  str(csse_doc['Combined_Key'])).replace(' ', '_').replace(':','-').replace('.', '_')
                            csse_doc['geo_point'] = {
                                'lat': csse_doc['Lat'],
                                'lon': csse_doc['Long_'],
                            }
                            doc['state'] = csse_doc
                            action = {
                                "_index": index,
                                "_type": "_doc",
                                "_id": _id,
                                "_source": doc
                            }
                            actions.append(action)
                    else:
                        print('No data for location: ', doc['location'], doc['@timestamp'].strftime("%Y-%m-%d"))
                        _id = (doc['continent'] + doc['iso_code'] + '_' + str(doc['@timestamp'])).replace(' ', '_').replace(':', '-')
                        doc['state'] = {}
                        action = {
                            "_index": index,
                            "_type": "_doc",
                            "_id": _id,
                            "_source": doc
                        }
                        actions.append(action)
                    if len(actions) >= 50:
                        push_bulk_data_to_es(actions=actions)
                        actions.clear()

        if len(actions) > 0:
            push_bulk_data_to_es(actions=actions)
            actions.clear()


    if __name__ == '__main__':
        paths = get_paths(owid_data_source_url)

        if not os.path.exists(paths['cleaned_file_path']):
            if not os.path.exists(paths['raw_file_path']):
                owid_df_csv = download_data(owid_data_source_url)
            else:
                owid_df_csv = paths['raw_file_path']
            if not owid_df_csv:
                sys.exit()
            owid_df = get_dataframe(owid_df_csv)
            cleaned_owid_df = get_cleaned_owid_df(owid_df)
            save_cleaned_df(cleaned_owid_df, paths['cleaned_file_path'])
        else:
            cleaned_owid_df = pd.read_csv(paths['cleaned_file_path'])
        process_bulk_es_data(cleaned_owid_df)


